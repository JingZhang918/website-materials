{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46ae2ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import pandas as pd\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.dqn import DQNTrainer\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ce208b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5e3431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PickLargest(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=float(\"-inf\"), high=float(\"inf\"), shape=(4, ))\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.obs = np.random.randn(4)\n",
    "        return self.obs\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = self.obs[action]\n",
    "        return self.obs, reward, True, {}\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return PickLargest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95809164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-20 14:47:46,598\tINFO trainer.py:753 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2021-11-20 14:47:46,599\tINFO dqn.py:143 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2021-11-20 14:47:46,600\tINFO trainer.py:772 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2021-11-20 14:47:48,164\tWARNING deprecation.py:39 -- DeprecationWarning: `ReplayBuffer(size)` has been deprecated. Use `ReplayBuffer(capacity)` instead. This will raise an error in the future!\n",
      "2021-11-20 14:47:48,166\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
      "2021-11-20 14:48:03,240\tWARNING deprecation.py:39 -- DeprecationWarning: `ReplayBuffer(size)` has been deprecated. Use `ReplayBuffer(capacity)` instead. This will raise an error in the future!\n",
      "2021-11-20 14:48:03,242\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
      "2021-11-20 14:48:17,546\tWARNING deprecation.py:39 -- DeprecationWarning: `ReplayBuffer(size)` has been deprecated. Use `ReplayBuffer(capacity)` instead. This will raise an error in the future!\n",
      "2021-11-20 14:48:17,549\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
      "2021-11-20 14:48:32,567\tWARNING deprecation.py:39 -- DeprecationWarning: `ReplayBuffer(size)` has been deprecated. Use `ReplayBuffer(capacity)` instead. This will raise an error in the future!\n",
      "2021-11-20 14:48:32,570\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "trajs = list()\n",
    "for trial in range(4):\n",
    "    ray.init(ignore_reinit_error=True)\n",
    "    register_env(\"PickLargest\", env_creator)\n",
    "    agent = DQNTrainer(\n",
    "        env=\"PickLargest\",\n",
    "        config={\"seed\": 666 if trial in [0, 1] else 999})\n",
    "\n",
    "    trajectory = list()\n",
    "    for _ in range(3):\n",
    "        r = agent.train()\n",
    "        trajectory.append(r[\"episode_reward_max\"])\n",
    "        trajectory.append(r[\"episode_reward_min\"])\n",
    "    trajs.append(trajectory)\n",
    "\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "932ca8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.7971520991646552, -2.857962229200296, 3.54639274942917, -2.7120852012876107, 2.9354710995698308, -3.427965628826908]\n",
      "[2.7971520991646552, -2.857962229200296, 3.54639274942917, -2.7120852012876107, 2.9354710995698308, -3.427965628826908]\n",
      "[3.1591653784582365, -3.354582245221421, 3.3737197863259074, -3.3048407485994082, 2.941864688280453, -3.5399449458396797]\n",
      "[3.1591653784582365, -3.354582245221421, 3.3737197863259074, -3.3048407485994082, 2.941864688280453, -3.5399449458396797]\n"
     ]
    }
   ],
   "source": [
    "for i in trajs:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b902282d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49128e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514b1a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5612fef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-20 14:48:47,794\tWARNING deprecation.py:39 -- DeprecationWarning: `ReplayBuffer(size)` has been deprecated. Use `ReplayBuffer(capacity)` instead. This will raise an error in the future!\n",
      "2021-11-20 14:48:47,797\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
      "2021-11-20 14:48:58,556\tWARNING deprecation.py:39 -- DeprecationWarning: `ReplayBuffer(size)` has been deprecated. Use `ReplayBuffer(capacity)` instead. This will raise an error in the future!\n",
      "2021-11-20 14:48:58,558\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
      "2021-11-20 14:49:09,204\tWARNING deprecation.py:39 -- DeprecationWarning: `ReplayBuffer(size)` has been deprecated. Use `ReplayBuffer(capacity)` instead. This will raise an error in the future!\n",
      "2021-11-20 14:49:09,207\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
      "2021-11-20 14:49:20,670\tWARNING deprecation.py:39 -- DeprecationWarning: `ReplayBuffer(size)` has been deprecated. Use `ReplayBuffer(capacity)` instead. This will raise an error in the future!\n",
      "2021-11-20 14:49:20,673\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "trajs = list()\n",
    "trajs_actions = list()\n",
    "trajs_rewards = []\n",
    "for trial in range(4):\n",
    "    ray.init(ignore_reinit_error=True)\n",
    "    \n",
    "    register_env(\"PickLargest\", env_creator)\n",
    "    agent = DQNTrainer(\n",
    "        env=\"PickLargest\",\n",
    "        config={\"seed\": 666 if trial in [0, 1] else 999})\n",
    "\n",
    "    trajectory = list()\n",
    "    for _ in range(2):\n",
    "        r = agent.train()\n",
    "        trajectory.append(r[\"episode_reward_max\"])\n",
    "        trajectory.append(r[\"episode_reward_min\"])\n",
    "    trajs.append(trajectory)\n",
    "    \n",
    "    actions = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "    env = PickLargest()\n",
    "    obs = env.reset()\n",
    "    while not done:\n",
    "        action = agent.compute_single_action(obs)\n",
    "        actions.append(action)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "    trajs_actions.append(actions)\n",
    "    trajs_rewards.append(rewards)\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26d5ac80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3], [3], [1], [1]]\n",
      "[[0.41575593666031874], [0.41575593666031874], [1.5788194892097913], [1.5788194892097913]]\n"
     ]
    }
   ],
   "source": [
    "print(trajs_actions)\n",
    "print(trajs_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d4c3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69114a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee4a21e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be476201",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-20 14:50:15 (running for 00:00:08.26)<br>Memory usage on this node: 3.7/13.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/6.62 GiB heap, 0.0/3.31 GiB objects<br>Current best trial: 2744e_00000 with episode_reward_mean=-0.02794726307170292 and parameters={'num_workers': 0, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 4, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 0.0005, 'train_batch_size': 32, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'PickLargest', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'EpsilonGreedy', 'initial_epsilon': 1.0, 'final_epsilon': 0.02, 'epsilon_timesteps': 10000}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'explore': False}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 1, 'timesteps_per_iteration': 1000, 'seed': 999, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=None, observation_space=None, action_space=None, config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': False, 'monitor': -1, 'target_network_update_freq': 500, 'buffer_size': 50000, 'store_buffer_in_checkpoints': False, 'replay_sequence_length': 1, 'lr_schedule': None, 'adam_epsilon': 1e-08, 'grad_clip': 40, 'learning_starts': 1000, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': True, 'hiddens': [256], 'double_q': True, 'n_step': 1, 'prioritized_replay': True, 'prioritized_replay_alpha': 0.6, 'prioritized_replay_beta': 0.4, 'final_prioritized_replay_beta': 0.4, 'prioritized_replay_beta_annealing_timesteps': 20000, 'prioritized_replay_eps': 1e-06, 'before_learn_on_batch': None, 'training_intensity': None, 'worker_side_prioritization': False}<br>Result logdir: /home/jing/ray_results/DQN_2021-11-20_14-50-07<br>Number of trials: 1/1 (1 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=491958)\u001b[0m [2021-11-20 14:50:16,030 E 491958 492064] raylet_client.cc:159: IOError: Broken pipe [RayletClient] Failed to disconnect from raylet.\n",
      "2021-11-20 14:50:16,133\tINFO tune.py:630 -- Total run time: 8.61 seconds (8.22 seconds for the tuning loop).\n",
      "2021-11-20 14:50:17,602\tWARNING deprecation.py:39 -- DeprecationWarning: `ReplayBuffer(size)` has been deprecated. Use `ReplayBuffer(capacity)` instead. This will raise an error in the future!\n",
      "2021-11-20 14:50:17,605\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
      "2021-11-20 14:50:17,712\tINFO trainable.py:417 -- Restored on 10.1.0.4 from checkpoint: /home/jing/ray_results/DQN_2021-11-20_14-50-07/DQN_PickLargest_2744e_00000_0_2021-11-20_14-50-07/checkpoint_000001/checkpoint-1\n",
      "2021-11-20 14:50:17,713\tINFO trainable.py:424 -- Current state after restoring: {'_iteration': 1, '_timesteps_total': 0, '_time_total': 2.39831805229187, '_episodes_total': 1000}\n"
     ]
    }
   ],
   "source": [
    "trajs_actions = list()\n",
    "trajs_rewards = []\n",
    "\n",
    "for trial in range(4):\n",
    "    ray.init(ignore_reinit_error=True)\n",
    "    register_env(\"PickLargest\", env_creator)\n",
    "    \n",
    "    #rap tune.ray inside\n",
    "    analysis = tune.run(\n",
    "        run_or_experiment=DQNTrainer\n",
    "        ,stop={'timesteps_total' : 3}\n",
    "        ,config={\n",
    "            'env': \"PickLargest\"\n",
    "            ,\"seed\" : 666 if trial in [0,1] else 999\n",
    "#             ,'lr': tune.grid_search([1e-1, 10])\n",
    "#             ,'vf_clip_param': 1e4\n",
    "        }\n",
    "        ,metric = \"episode_reward_mean\"\n",
    "        ,mode = 'max'\n",
    "        ,checkpoint_at_end = True\n",
    "        ,verbose = 1\n",
    "    )\n",
    "    \n",
    "    agent = DQNTrainer(config=analysis.best_config, env=\"PickLargest\")\n",
    "    agent.restore(analysis.best_checkpoint)\n",
    "    \n",
    "    actions = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "    env = PickLargest()\n",
    "    obs = env.reset()\n",
    "    while not done:\n",
    "        action = agent.compute_single_action(obs)\n",
    "        actions.append(action)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "    trajs_actions.append(actions)\n",
    "    trajs_rewards.append(rewards)\n",
    "    ray.shutdown()  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07f56b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3], [3], [0], [0]]\n",
      "[[0.7990245823262375], [0.7990245823262375], [0.43561831700300296], [0.43561831700300296]]\n"
     ]
    }
   ],
   "source": [
    "print(trajs_actions)\n",
    "print(trajs_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90967ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4600b53a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3917a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6b20c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149ab7e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
